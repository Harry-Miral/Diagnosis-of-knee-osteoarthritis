{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "452b9946",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-05T12:59:35.081622Z",
     "start_time": "2022-07-05T12:59:35.077621Z"
    }
   },
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8cf216bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T13:19:46.997604Z",
     "start_time": "2022-07-07T13:19:42.857680Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "from pandas import read_csv, unique\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras import optimizers\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import mode\n",
    "from scipy import interp\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,roc_curve,auc\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import metrics\n",
    "\n",
    "from tensorflow import stack\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, BatchNormalization, MaxPool1D, Reshape, Activation\n",
    "from keras.layers import Conv1D, LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import statsmodels.stats.api as sms\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56be0860",
   "metadata": {},
   "source": [
    "# Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4351ece",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T13:19:47.013608Z",
     "start_time": "2022-07-07T13:19:46.998607Z"
    }
   },
   "outputs": [],
   "source": [
    "def read_data(filepath):\n",
    "    df = read_csv(filepath, header=None, names=['user-id',\n",
    "                                               'activity',\n",
    "                                               'timestamp',\n",
    "                                               'sex',\n",
    "                                               'age',\n",
    "                                               'BMI',\n",
    "                                               'A',\n",
    "                                               'B',\n",
    "                                               'C',\n",
    "                                               'X',\n",
    "                                               'Y',\n",
    "                                               'Z'])\n",
    "    ## removing ';' from last column and converting it to float\n",
    "    df['Z'].replace(regex=True, inplace=True, to_replace=r';', value=r'')\n",
    "    df['Z'] = df['Z'].apply(convert_to_float)\n",
    "#     df.dropna(axis=0, how='any', inplace=True)\n",
    "    return df\n",
    "\n",
    "def convert_to_float(x):\n",
    "    try:\n",
    "        return np.float64(x)\n",
    "    except:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe14cbaa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T13:19:48.655211Z",
     "start_time": "2022-07-07T13:19:47.014608Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user-id</th>\n",
       "      <th>activity</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.842623</td>\n",
       "      <td>170.059424</td>\n",
       "      <td>171.885206</td>\n",
       "      <td>164.917391</td>\n",
       "      <td>165.412991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.642798</td>\n",
       "      <td>170.228278</td>\n",
       "      <td>172.175605</td>\n",
       "      <td>164.844014</td>\n",
       "      <td>165.337399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.442973</td>\n",
       "      <td>170.397132</td>\n",
       "      <td>172.466004</td>\n",
       "      <td>164.770636</td>\n",
       "      <td>165.261807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.392961</td>\n",
       "      <td>170.137725</td>\n",
       "      <td>171.981197</td>\n",
       "      <td>164.342390</td>\n",
       "      <td>165.068027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>23.342949</td>\n",
       "      <td>169.878317</td>\n",
       "      <td>171.496389</td>\n",
       "      <td>163.914143</td>\n",
       "      <td>164.874246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285595</th>\n",
       "      <td>357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>795</td>\n",
       "      <td>395.0</td>\n",
       "      <td>53.054386</td>\n",
       "      <td>101.282433</td>\n",
       "      <td>98.437307</td>\n",
       "      <td>91.694820</td>\n",
       "      <td>95.570696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285596</th>\n",
       "      <td>357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>796</td>\n",
       "      <td>396.0</td>\n",
       "      <td>51.566923</td>\n",
       "      <td>104.802220</td>\n",
       "      <td>100.029568</td>\n",
       "      <td>98.373605</td>\n",
       "      <td>98.261446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285597</th>\n",
       "      <td>357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>797</td>\n",
       "      <td>397.0</td>\n",
       "      <td>50.444408</td>\n",
       "      <td>106.489055</td>\n",
       "      <td>101.999649</td>\n",
       "      <td>101.329230</td>\n",
       "      <td>100.532071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285598</th>\n",
       "      <td>357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>798</td>\n",
       "      <td>398.0</td>\n",
       "      <td>50.424259</td>\n",
       "      <td>106.749087</td>\n",
       "      <td>102.629601</td>\n",
       "      <td>101.878083</td>\n",
       "      <td>101.378611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285599</th>\n",
       "      <td>357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>799</td>\n",
       "      <td>399.0</td>\n",
       "      <td>50.404110</td>\n",
       "      <td>107.009119</td>\n",
       "      <td>103.259553</td>\n",
       "      <td>102.426936</td>\n",
       "      <td>102.225152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>285600 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user-id activity  timestamp      A          B           C           X  \\\n",
       "0             1      Yes          0    0.0  23.842623  170.059424  171.885206   \n",
       "1             1      Yes          1    1.0  23.642798  170.228278  172.175605   \n",
       "2             1      Yes          2    2.0  23.442973  170.397132  172.466004   \n",
       "3             1      Yes          3    3.0  23.392961  170.137725  171.981197   \n",
       "4             1      Yes          4    4.0  23.342949  169.878317  171.496389   \n",
       "...         ...      ...        ...    ...        ...         ...         ...   \n",
       "285595      357      Yes        795  395.0  53.054386  101.282433   98.437307   \n",
       "285596      357      Yes        796  396.0  51.566923  104.802220  100.029568   \n",
       "285597      357      Yes        797  397.0  50.444408  106.489055  101.999649   \n",
       "285598      357      Yes        798  398.0  50.424259  106.749087  102.629601   \n",
       "285599      357      Yes        799  399.0  50.404110  107.009119  103.259553   \n",
       "\n",
       "                 Y           Z  \n",
       "0       164.917391  165.412991  \n",
       "1       164.844014  165.337399  \n",
       "2       164.770636  165.261807  \n",
       "3       164.342390  165.068027  \n",
       "4       163.914143  164.874246  \n",
       "...            ...         ...  \n",
       "285595   91.694820   95.570696  \n",
       "285596   98.373605   98.261446  \n",
       "285597  101.329230  100.532071  \n",
       "285598  101.878083  101.378611  \n",
       "285599  102.426936  102.225152  \n",
       "\n",
       "[285600 rows x 9 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = read_data('Dataset/Angel_and_Baseline/Angel_data_order.txt')\n",
    "df = df.drop(labels=['sex', 'age','BMI'], axis=1) #delete baseline data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62cfc0f2",
   "metadata": {},
   "source": [
    "## Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78699eb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T13:19:50.436290Z",
     "start_time": "2022-07-07T13:19:50.281071Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user-id</th>\n",
       "      <th>activity</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>activityEncode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>23.842623</td>\n",
       "      <td>170.059424</td>\n",
       "      <td>171.885206</td>\n",
       "      <td>164.917391</td>\n",
       "      <td>165.412991</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23.642798</td>\n",
       "      <td>170.228278</td>\n",
       "      <td>172.175605</td>\n",
       "      <td>164.844014</td>\n",
       "      <td>165.337399</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>23.442973</td>\n",
       "      <td>170.397132</td>\n",
       "      <td>172.466004</td>\n",
       "      <td>164.770636</td>\n",
       "      <td>165.261807</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>23.392961</td>\n",
       "      <td>170.137725</td>\n",
       "      <td>171.981197</td>\n",
       "      <td>164.342390</td>\n",
       "      <td>165.068027</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>23.342949</td>\n",
       "      <td>169.878317</td>\n",
       "      <td>171.496389</td>\n",
       "      <td>163.914143</td>\n",
       "      <td>164.874246</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285595</th>\n",
       "      <td>357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>795</td>\n",
       "      <td>395.0</td>\n",
       "      <td>53.054386</td>\n",
       "      <td>101.282433</td>\n",
       "      <td>98.437307</td>\n",
       "      <td>91.694820</td>\n",
       "      <td>95.570696</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285596</th>\n",
       "      <td>357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>796</td>\n",
       "      <td>396.0</td>\n",
       "      <td>51.566923</td>\n",
       "      <td>104.802220</td>\n",
       "      <td>100.029568</td>\n",
       "      <td>98.373605</td>\n",
       "      <td>98.261446</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285597</th>\n",
       "      <td>357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>797</td>\n",
       "      <td>397.0</td>\n",
       "      <td>50.444408</td>\n",
       "      <td>106.489055</td>\n",
       "      <td>101.999649</td>\n",
       "      <td>101.329230</td>\n",
       "      <td>100.532071</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285598</th>\n",
       "      <td>357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>798</td>\n",
       "      <td>398.0</td>\n",
       "      <td>50.424259</td>\n",
       "      <td>106.749087</td>\n",
       "      <td>102.629601</td>\n",
       "      <td>101.878083</td>\n",
       "      <td>101.378611</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285599</th>\n",
       "      <td>357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>799</td>\n",
       "      <td>399.0</td>\n",
       "      <td>50.404110</td>\n",
       "      <td>107.009119</td>\n",
       "      <td>103.259553</td>\n",
       "      <td>102.426936</td>\n",
       "      <td>102.225152</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>285600 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user-id activity  timestamp      A          B           C           X  \\\n",
       "0             1      Yes          0    0.0  23.842623  170.059424  171.885206   \n",
       "1             1      Yes          1    1.0  23.642798  170.228278  172.175605   \n",
       "2             1      Yes          2    2.0  23.442973  170.397132  172.466004   \n",
       "3             1      Yes          3    3.0  23.392961  170.137725  171.981197   \n",
       "4             1      Yes          4    4.0  23.342949  169.878317  171.496389   \n",
       "...         ...      ...        ...    ...        ...         ...         ...   \n",
       "285595      357      Yes        795  395.0  53.054386  101.282433   98.437307   \n",
       "285596      357      Yes        796  396.0  51.566923  104.802220  100.029568   \n",
       "285597      357      Yes        797  397.0  50.444408  106.489055  101.999649   \n",
       "285598      357      Yes        798  398.0  50.424259  106.749087  102.629601   \n",
       "285599      357      Yes        799  399.0  50.404110  107.009119  103.259553   \n",
       "\n",
       "                 Y           Z  activityEncode  \n",
       "0       164.917391  165.412991               1  \n",
       "1       164.844014  165.337399               1  \n",
       "2       164.770636  165.261807               1  \n",
       "3       164.342390  165.068027               1  \n",
       "4       163.914143  164.874246               1  \n",
       "...            ...         ...             ...  \n",
       "285595   91.694820   95.570696               1  \n",
       "285596   98.373605   98.261446               1  \n",
       "285597  101.329230  100.532071               1  \n",
       "285598  101.878083  101.378611               1  \n",
       "285599  102.426936  102.225152               1  \n",
       "\n",
       "[285600 rows x 10 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encode = LabelEncoder()\n",
    "df['activityEncode'] = label_encode.fit_transform(df['activity'].values.ravel())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3be5152",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-04T16:53:26.834976Z",
     "start_time": "2022-07-04T16:53:26.823973Z"
    }
   },
   "source": [
    "## Linear Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7951cd31",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T13:19:50.484301Z",
     "start_time": "2022-07-07T13:19:50.437290Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user-id           0\n",
       "activity          0\n",
       "timestamp         0\n",
       "A                 0\n",
       "B                 0\n",
       "C                 0\n",
       "X                 0\n",
       "Y                 0\n",
       "Z                 0\n",
       "activityEncode    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6d9e58d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T13:19:50.532284Z",
     "start_time": "2022-07-07T13:19:50.485302Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<scipy.interpolate._interpolate.interp1d at 0x214738db330>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolation_fn = interp1d(df['activityEncode'] ,df['Z'], kind='linear')\n",
    "interpolation_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f8ea8cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T13:19:50.548288Z",
     "start_time": "2022-07-07T13:19:50.534285Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "null_list = df[df['Z'].isnull()].index.tolist()\n",
    "null_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f002bca0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T13:19:50.580295Z",
     "start_time": "2022-07-07T13:19:50.549289Z"
    }
   },
   "outputs": [],
   "source": [
    "for i in null_list:\n",
    "    y = df['activityEncode'][i]\n",
    "    value = interpolation_fn(y)\n",
    "    df['Z']=df['Z'].fillna(value)\n",
    "    print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4349e1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-07T13:19:50.627795Z",
     "start_time": "2022-07-07T13:19:50.581295Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user-id           0\n",
       "activity          0\n",
       "timestamp         0\n",
       "A                 0\n",
       "B                 0\n",
       "C                 0\n",
       "X                 0\n",
       "Y                 0\n",
       "Z                 0\n",
       "activityEncode    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69abe57c",
   "metadata": {},
   "source": [
    "## Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a24a047f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user-id</th>\n",
       "      <th>activity</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>activityEncode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209557</td>\n",
       "      <td>0.924607</td>\n",
       "      <td>0.933212</td>\n",
       "      <td>0.888332</td>\n",
       "      <td>0.886692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.206448</td>\n",
       "      <td>0.925895</td>\n",
       "      <td>0.935667</td>\n",
       "      <td>0.887787</td>\n",
       "      <td>0.886036</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005013</td>\n",
       "      <td>0.203339</td>\n",
       "      <td>0.927182</td>\n",
       "      <td>0.938121</td>\n",
       "      <td>0.887241</td>\n",
       "      <td>0.885379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.202561</td>\n",
       "      <td>0.925204</td>\n",
       "      <td>0.934024</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.883695</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010025</td>\n",
       "      <td>0.201782</td>\n",
       "      <td>0.923227</td>\n",
       "      <td>0.929926</td>\n",
       "      <td>0.880875</td>\n",
       "      <td>0.882012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285595</th>\n",
       "      <td>357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>795</td>\n",
       "      <td>0.989975</td>\n",
       "      <td>0.664060</td>\n",
       "      <td>0.400242</td>\n",
       "      <td>0.312480</td>\n",
       "      <td>0.344084</td>\n",
       "      <td>0.279896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285596</th>\n",
       "      <td>357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>796</td>\n",
       "      <td>0.992481</td>\n",
       "      <td>0.640917</td>\n",
       "      <td>0.427078</td>\n",
       "      <td>0.325937</td>\n",
       "      <td>0.393726</td>\n",
       "      <td>0.303273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285597</th>\n",
       "      <td>357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>797</td>\n",
       "      <td>0.994987</td>\n",
       "      <td>0.623451</td>\n",
       "      <td>0.439938</td>\n",
       "      <td>0.342587</td>\n",
       "      <td>0.415694</td>\n",
       "      <td>0.323001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285598</th>\n",
       "      <td>357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>798</td>\n",
       "      <td>0.997494</td>\n",
       "      <td>0.623138</td>\n",
       "      <td>0.441921</td>\n",
       "      <td>0.347911</td>\n",
       "      <td>0.419774</td>\n",
       "      <td>0.330355</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285599</th>\n",
       "      <td>357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>799</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.622824</td>\n",
       "      <td>0.443904</td>\n",
       "      <td>0.353234</td>\n",
       "      <td>0.423853</td>\n",
       "      <td>0.337710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>285600 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user-id activity  timestamp         A         B         C         X  \\\n",
       "0             1      Yes          0  0.000000  0.209557  0.924607  0.933212   \n",
       "1             1      Yes          1  0.002506  0.206448  0.925895  0.935667   \n",
       "2             1      Yes          2  0.005013  0.203339  0.927182  0.938121   \n",
       "3             1      Yes          3  0.007519  0.202561  0.925204  0.934024   \n",
       "4             1      Yes          4  0.010025  0.201782  0.923227  0.929926   \n",
       "...         ...      ...        ...       ...       ...       ...       ...   \n",
       "285595      357      Yes        795  0.989975  0.664060  0.400242  0.312480   \n",
       "285596      357      Yes        796  0.992481  0.640917  0.427078  0.325937   \n",
       "285597      357      Yes        797  0.994987  0.623451  0.439938  0.342587   \n",
       "285598      357      Yes        798  0.997494  0.623138  0.441921  0.347911   \n",
       "285599      357      Yes        799  1.000000  0.622824  0.443904  0.353234   \n",
       "\n",
       "               Y         Z  activityEncode  \n",
       "0       0.888332  0.886692               1  \n",
       "1       0.887787  0.886036               1  \n",
       "2       0.887241  0.885379               1  \n",
       "3       0.884058  0.883695               1  \n",
       "4       0.880875  0.882012               1  \n",
       "...          ...       ...             ...  \n",
       "285595  0.344084  0.279896               1  \n",
       "285596  0.393726  0.303273               1  \n",
       "285597  0.415694  0.323001               1  \n",
       "285598  0.419774  0.330355               1  \n",
       "285599  0.423853  0.337710               1  \n",
       "\n",
       "[285600 rows x 10 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df['age'] = (df['age']-df['age'].min())/(df['age'].max()-df['age'].min())\n",
    "#df['BMI'] = (df['BMI']-df['BMI'].min())/(df['BMI'].max()-df['BMI'].min())\n",
    "df['A'] = (df['A']-df['A'].min())/(df['A'].max()-df['A'].min())\n",
    "df['B'] = (df['B']-df['B'].min())/(df['B'].max()-df['B'].min())\n",
    "df['C'] = (df['C']-df['C'].min())/(df['C'].max()-df['C'].min())\n",
    "df['X'] = (df['X']-df['X'].min())/(df['X'].max()-df['X'].min())\n",
    "df['Y'] = (df['Y']-df['Y'].min())/(df['Y'].max()-df['Y'].min())\n",
    "df['Z'] = (df['Z']-df['Z'].min())/(df['Z'].max()-df['Z'].min())\n",
    "#df['sex'] = (df['sex']-df['sex'].min())/(df['sex'].max()-df['sex'].min())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a31ec3ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segments(df, time_steps, step, label_name):\n",
    "    N_FEATURES = 6\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for i in range(0, len(df) - time_steps, step):\n",
    "        xs = df['X'].values[i:i+time_steps]\n",
    "        ys = df['Y'].values[i:i+time_steps]\n",
    "        zs = df['Z'].values[i:i+time_steps]\n",
    "        aas = df['A'].values[i:i+time_steps]\n",
    "        bs = df['B'].values[i:i+time_steps]\n",
    "        cs = df['C'].values[i:i+time_steps]\n",
    "        #sexs = df['sex'].values[i:i+time_steps]\n",
    "        #ages = df['age'].values[i:i+time_steps]\n",
    "        #bmis = df['BMI'].values[i:i+time_steps]\n",
    "        label = mode(df[label_name][i:i+time_steps])[0][0]\n",
    "        segments.append([aas,bs,cs,xs, ys, zs])\n",
    "        labels.append(label)\n",
    "    reshaped_segments = np.asarray(segments, dtype=np.float32).reshape(-1, time_steps, N_FEATURES)\n",
    "    labels = np.asarray(labels)\n",
    "    return reshaped_segments, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "987cccea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user-id</th>\n",
       "      <th>activity</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "      <th>Z</th>\n",
       "      <th>activityEncode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.209557</td>\n",
       "      <td>0.924607</td>\n",
       "      <td>0.933212</td>\n",
       "      <td>0.888332</td>\n",
       "      <td>0.886692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>1</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.206448</td>\n",
       "      <td>0.925895</td>\n",
       "      <td>0.935667</td>\n",
       "      <td>0.887787</td>\n",
       "      <td>0.886036</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>2</td>\n",
       "      <td>0.005013</td>\n",
       "      <td>0.203339</td>\n",
       "      <td>0.927182</td>\n",
       "      <td>0.938121</td>\n",
       "      <td>0.887241</td>\n",
       "      <td>0.885379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>3</td>\n",
       "      <td>0.007519</td>\n",
       "      <td>0.202561</td>\n",
       "      <td>0.925204</td>\n",
       "      <td>0.934024</td>\n",
       "      <td>0.884058</td>\n",
       "      <td>0.883695</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>4</td>\n",
       "      <td>0.010025</td>\n",
       "      <td>0.201782</td>\n",
       "      <td>0.923227</td>\n",
       "      <td>0.929926</td>\n",
       "      <td>0.880875</td>\n",
       "      <td>0.882012</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285595</th>\n",
       "      <td>357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>795</td>\n",
       "      <td>0.989975</td>\n",
       "      <td>0.664060</td>\n",
       "      <td>0.400242</td>\n",
       "      <td>0.312480</td>\n",
       "      <td>0.344084</td>\n",
       "      <td>0.279896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285596</th>\n",
       "      <td>357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>796</td>\n",
       "      <td>0.992481</td>\n",
       "      <td>0.640917</td>\n",
       "      <td>0.427078</td>\n",
       "      <td>0.325937</td>\n",
       "      <td>0.393726</td>\n",
       "      <td>0.303273</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285597</th>\n",
       "      <td>357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>797</td>\n",
       "      <td>0.994987</td>\n",
       "      <td>0.623451</td>\n",
       "      <td>0.439938</td>\n",
       "      <td>0.342587</td>\n",
       "      <td>0.415694</td>\n",
       "      <td>0.323001</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285598</th>\n",
       "      <td>357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>798</td>\n",
       "      <td>0.997494</td>\n",
       "      <td>0.623138</td>\n",
       "      <td>0.441921</td>\n",
       "      <td>0.347911</td>\n",
       "      <td>0.419774</td>\n",
       "      <td>0.330355</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285599</th>\n",
       "      <td>357</td>\n",
       "      <td>Yes</td>\n",
       "      <td>799</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.622824</td>\n",
       "      <td>0.443904</td>\n",
       "      <td>0.353234</td>\n",
       "      <td>0.423853</td>\n",
       "      <td>0.337710</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>285600 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user-id activity  timestamp         A         B         C         X  \\\n",
       "0             1      Yes          0  0.000000  0.209557  0.924607  0.933212   \n",
       "1             1      Yes          1  0.002506  0.206448  0.925895  0.935667   \n",
       "2             1      Yes          2  0.005013  0.203339  0.927182  0.938121   \n",
       "3             1      Yes          3  0.007519  0.202561  0.925204  0.934024   \n",
       "4             1      Yes          4  0.010025  0.201782  0.923227  0.929926   \n",
       "...         ...      ...        ...       ...       ...       ...       ...   \n",
       "285595      357      Yes        795  0.989975  0.664060  0.400242  0.312480   \n",
       "285596      357      Yes        796  0.992481  0.640917  0.427078  0.325937   \n",
       "285597      357      Yes        797  0.994987  0.623451  0.439938  0.342587   \n",
       "285598      357      Yes        798  0.997494  0.623138  0.441921  0.347911   \n",
       "285599      357      Yes        799  1.000000  0.622824  0.443904  0.353234   \n",
       "\n",
       "               Y         Z  activityEncode  \n",
       "0       0.888332  0.886692               1  \n",
       "1       0.887787  0.886036               1  \n",
       "2       0.887241  0.885379               1  \n",
       "3       0.884058  0.883695               1  \n",
       "4       0.880875  0.882012               1  \n",
       "...          ...       ...             ...  \n",
       "285595  0.344084  0.279896               1  \n",
       "285596  0.393726  0.303273               1  \n",
       "285597  0.415694  0.323001               1  \n",
       "285598  0.419774  0.330355               1  \n",
       "285599  0.423853  0.337710               1  \n",
       "\n",
       "[285600 rows x 10 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b783875",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIME_PERIOD = 80\n",
    "STEP_DISTANCE = 40\n",
    "LABEL = 'activityEncode'\n",
    "\n",
    "df1=df[df['user-id']>70]\n",
    "df2=df[df['user-id']>140]\n",
    "df3=df[df['user-id']>210]\n",
    "df4=df[df['user-id']>280]\n",
    "\n",
    "a1=df.shape[0]\n",
    "b2=df1.shape[0]\n",
    "c3=df2.shape[0]\n",
    "d4=df3.shape[0]\n",
    "e5=df4.shape[0]\n",
    "\n",
    "df_test0 = df.iloc[0:a1-b2,:]\n",
    "df_train0 = df.iloc[a1-b2:a1,:]\n",
    "x_train0, y_train0 = segments(df_train0, TIME_PERIOD, STEP_DISTANCE, LABEL)\n",
    "x_test0, y_test0 = segments(df_test0, TIME_PERIOD, STEP_DISTANCE, LABEL)\n",
    "time_period, sensors = x_train0.shape[1], x_train0.shape[2]\n",
    "num_classes = label_encode.classes_.size\n",
    "input_shape = time_period * sensors\n",
    "x_train0 = x_train0.reshape(x_train0.shape[0], input_shape)\n",
    "x_train0 = x_train0.astype('float32')\n",
    "y_train0=np.asarray(y_train0).astype('float32').reshape((-1,1))\n",
    "time_period, sensors = x_test0.shape[1], x_test0.shape[2]\n",
    "num_classes = label_encode.classes_.size\n",
    "input_shape = time_period * sensors\n",
    "x_test0 = x_test0.reshape(x_test0.shape[0], input_shape)\n",
    "x_test0 = x_test0.astype('float32')\n",
    "y_test0=np.asarray(y_test0).astype('float32').reshape((-1,1))\n",
    "\n",
    "df_test1 = df.iloc[a1-b2:a1-c3,:]\n",
    "df_train1 = pd.concat([df_test0,df.iloc[a1-c3:a1,]])\n",
    "x_train1, y_train1 = segments(df_train0, TIME_PERIOD, STEP_DISTANCE, LABEL)\n",
    "x_test1, y_test1 = segments(df_test0, TIME_PERIOD, STEP_DISTANCE, LABEL)\n",
    "time_period, sensors = x_train1.shape[1], x_train1.shape[2]\n",
    "num_classes = label_encode.classes_.size\n",
    "input_shape = time_period * sensors\n",
    "x_train1 = x_train1.reshape(x_train1.shape[0], input_shape)\n",
    "x_train1 = x_train1.astype('float32')\n",
    "y_train1=np.asarray(y_train1).astype('float32').reshape((-1,1))\n",
    "time_period, sensors = x_test1.shape[1], x_test1.shape[2]\n",
    "num_classes = label_encode.classes_.size\n",
    "input_shape = time_period * sensors\n",
    "x_test1 = x_test1.reshape(x_test1.shape[0], input_shape)\n",
    "x_test1 = x_test1.astype('float32')\n",
    "y_test1=np.asarray(y_test1).astype('float32').reshape((-1,1))\n",
    "\n",
    "df_test2 = df.iloc[a1-c3:a1-d4,:]\n",
    "df_train2 = pd.concat([df.iloc[0:a1-c3,:],df.iloc[a1-d4:a1,]])\n",
    "x_train2, y_train2 = segments(df_train0, TIME_PERIOD, STEP_DISTANCE, LABEL)\n",
    "x_test2, y_test2 = segments(df_test0, TIME_PERIOD, STEP_DISTANCE, LABEL)\n",
    "time_period, sensors = x_train2.shape[1], x_train2.shape[2]\n",
    "num_classes = label_encode.classes_.size\n",
    "input_shape = time_period * sensors\n",
    "x_train2 = x_train2.reshape(x_train2.shape[0], input_shape)\n",
    "x_train2 = x_train2.astype('float32')\n",
    "y_train2=np.asarray(y_train2).astype('float32').reshape((-1,1))\n",
    "time_period, sensors = x_test2.shape[1], x_test2.shape[2]\n",
    "num_classes = label_encode.classes_.size\n",
    "input_shape = time_period * sensors\n",
    "x_test2 = x_test2.reshape(x_test2.shape[0], input_shape)\n",
    "x_test2 = x_test2.astype('float32')\n",
    "y_test2=np.asarray(y_test2).astype('float32').reshape((-1,1))\n",
    "\n",
    "df_test3 = df.iloc[a1-d4:a1-e5,:]\n",
    "df_train3 = pd.concat([df.iloc[0:a1-d4,:],df.iloc[a1-e5:a1,]])\n",
    "x_train3, y_train3 = segments(df_train0, TIME_PERIOD, STEP_DISTANCE, LABEL)\n",
    "x_test3, y_test3 = segments(df_test0, TIME_PERIOD, STEP_DISTANCE, LABEL)\n",
    "time_period, sensors = x_train3.shape[1], x_train3.shape[2]\n",
    "num_classes = label_encode.classes_.size\n",
    "input_shape = time_period * sensors\n",
    "x_train3 = x_train3.reshape(x_train3.shape[0], input_shape)\n",
    "x_train3 = x_train3.astype('float32')\n",
    "y_train3=np.asarray(y_train3).astype('float32').reshape((-1,1))\n",
    "time_period, sensors = x_test3.shape[1], x_test3.shape[2]\n",
    "num_classes = label_encode.classes_.size\n",
    "input_shape = time_period * sensors\n",
    "x_test3 = x_test3.reshape(x_test3.shape[0], input_shape)\n",
    "x_test3 = x_test3.astype('float32')\n",
    "y_test3=np.asarray(y_test3).astype('float32').reshape((-1,1))\n",
    "\n",
    "df_test4 = df.iloc[a1-e5:a1,:]\n",
    "df_train4 = df.iloc[0:a1-e5,:]\n",
    "x_train4, y_train4 = segments(df_train0, TIME_PERIOD, STEP_DISTANCE, LABEL)\n",
    "x_test4, y_test4 = segments(df_test0, TIME_PERIOD, STEP_DISTANCE, LABEL)\n",
    "time_period, sensors = x_train4.shape[1], x_train4.shape[2]\n",
    "num_classes = label_encode.classes_.size\n",
    "input_shape = time_period * sensors\n",
    "x_train4 = x_train4.reshape(x_train4.shape[0], input_shape)\n",
    "x_train4 = x_train4.astype('float32')\n",
    "y_train4=np.asarray(y_train4).astype('float32').reshape((-1,1))\n",
    "time_period, sensors = x_test4.shape[1], x_test4.shape[2]\n",
    "num_classes = label_encode.classes_.size\n",
    "input_shape = time_period * sensors\n",
    "x_test4 = x_test4.reshape(x_test4.shape[0], input_shape)\n",
    "x_test4 = x_test4.astype('float32')\n",
    "y_test4=np.asarray(y_test4).astype('float32').reshape((-1,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "40c9e344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 480, 32)           4352      \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 480, 32)           8320      \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 1, 480, 32)        0         \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 1, 240, 64)        4160      \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 240, 64)           0         \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 60, 64)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 59, 192)           24768     \n",
      "                                                                 \n",
      " reshape_2 (Reshape)         (None, 59, 192)           0         \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 192)              0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 192)              768       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 193       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42,561\n",
      "Trainable params: 42,177\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(32, return_sequences=True, input_shape=(input_shape,1), activation='relu'))\n",
    "model.add(LSTM(32,return_sequences=True, activation='relu'))\n",
    "model.add(Reshape((1, 480, 32)))\n",
    "model.add(Conv1D(filters=64,kernel_size=2, activation='relu', strides=2))\n",
    "model.add(Reshape((240, 64)))\n",
    "model.add(MaxPool1D(pool_size=4, padding='same'))\n",
    "model.add(Conv1D(filters=192, kernel_size=2, activation='relu', strides=1))\n",
    "model.add(Reshape((59, 192)))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(BatchNormalization(epsilon=1e-06))\n",
    "model.add(Dense(1))#改动\n",
    "model.add(Activation('sigmoid'))#原来是softmax\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1538a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lr_metric(optimizer):  # printing the value of the learning rate\n",
    "    def lr(y_true, y_pred):\n",
    "        return optimizer.lr\n",
    "    return lr\n",
    "\n",
    "def step_decay(epoch):\n",
    "\tinitial_lrate = 0.001\n",
    "\tdrop = 0.5\n",
    "\tepochs_drop = 5.0\n",
    "\tlrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "\treturn lrate\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "sgd = SGD(lr=0.0, momentum=0.9, decay=0.0, nesterov=False)\n",
    "adam=optimizers.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, decay=0.0, amsgrad=False)\n",
    "#lr_metric = get_lr_metric(adam)\n",
    "lr_metric = get_lr_metric(adam)\n",
    "model.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy',lr_metric])\n",
    "model1 = Sequential()\n",
    "model1.add(LSTM(32, return_sequences=True, input_shape=(input_shape,1), activation='relu'))\n",
    "model1.add(LSTM(32,return_sequences=True, activation='relu'))\n",
    "model1.add(Reshape((1, 480, 32)))\n",
    "model1.add(Conv1D(filters=64,kernel_size=2, activation='relu', strides=2))\n",
    "model1.add(Reshape((240, 64)))\n",
    "model1.add(MaxPool1D(pool_size=4, padding='same'))\n",
    "model1.add(Conv1D(filters=192, kernel_size=2, activation='relu', strides=1))\n",
    "model1.add(Reshape((59, 192)))\n",
    "model1.add(GlobalAveragePooling1D())\n",
    "model1.add(BatchNormalization(epsilon=1e-06))\n",
    "model1.add(Dense(1))\n",
    "model1.add(Activation('sigmoid'))\n",
    "model1.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy',lr_metric])\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(LSTM(32, return_sequences=True, input_shape=(input_shape,1), activation='relu'))\n",
    "model2.add(LSTM(32,return_sequences=True, activation='relu'))\n",
    "model2.add(Reshape((1, 480, 32)))\n",
    "model2.add(Conv1D(filters=64,kernel_size=2, activation='relu', strides=2))\n",
    "model2.add(Reshape((240, 64)))\n",
    "model2.add(MaxPool1D(pool_size=4, padding='same'))\n",
    "model2.add(Conv1D(filters=192, kernel_size=2, activation='relu', strides=1))\n",
    "model2.add(Reshape((59, 192)))\n",
    "model2.add(GlobalAveragePooling1D())\n",
    "model2.add(BatchNormalization(epsilon=1e-06))\n",
    "model2.add(Dense(1))\n",
    "model2.add(Activation('sigmoid'))\n",
    "model2.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy',lr_metric])\n",
    "\n",
    "model3 = Sequential()\n",
    "model3.add(LSTM(32, return_sequences=True, input_shape=(input_shape,1), activation='relu'))\n",
    "model3.add(LSTM(32,return_sequences=True, activation='relu'))\n",
    "model3.add(Reshape((1, 480, 32)))\n",
    "model3.add(Conv1D(filters=64,kernel_size=2, activation='relu', strides=2))\n",
    "model3.add(Reshape((240, 64)))\n",
    "model3.add(MaxPool1D(pool_size=4, padding='same'))\n",
    "model3.add(Conv1D(filters=192, kernel_size=2, activation='relu', strides=1))\n",
    "model3.add(Reshape((59, 192)))\n",
    "model3.add(GlobalAveragePooling1D())\n",
    "model3.add(BatchNormalization(epsilon=1e-06))\n",
    "model3.add(Dense(1))\n",
    "model3.add(Activation('sigmoid'))\n",
    "model3.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy',lr_metric])\n",
    "\n",
    "model4 = Sequential()\n",
    "model4.add(LSTM(32, return_sequences=True, input_shape=(input_shape,1), activation='relu'))\n",
    "model4.add(LSTM(32,return_sequences=True, activation='relu'))\n",
    "model4.add(Reshape((1, 480, 32)))\n",
    "model4.add(Conv1D(filters=64,kernel_size=2, activation='relu', strides=2))\n",
    "model4.add(Reshape((240, 64)))\n",
    "model4.add(MaxPool1D(pool_size=4, padding='same'))\n",
    "model4.add(Conv1D(filters=192, kernel_size=2, activation='relu', strides=1))\n",
    "model4.add(Reshape((59, 192)))\n",
    "model4.add(GlobalAveragePooling1D())\n",
    "model4.add(BatchNormalization(epsilon=1e-06))\n",
    "model4.add(Dense(1))\n",
    "model4.add(Activation('sigmoid'))\n",
    "model4.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy',lr_metric])\n",
    "\n",
    "model5 = Sequential()\n",
    "model5.add(LSTM(32, return_sequences=True, input_shape=(input_shape,1), activation='relu'))\n",
    "model5.add(LSTM(32,return_sequences=True, activation='relu'))\n",
    "model5.add(Reshape((1, 480, 32)))\n",
    "model5.add(Conv1D(filters=64,kernel_size=2, activation='relu', strides=2))\n",
    "model5.add(Reshape((240, 64)))\n",
    "model5.add(MaxPool1D(pool_size=4, padding='same'))\n",
    "model5.add(Conv1D(filters=192, kernel_size=2, activation='relu', strides=1))\n",
    "model5.add(Reshape((59, 192)))\n",
    "model5.add(GlobalAveragePooling1D())\n",
    "model5.add(BatchNormalization(epsilon=1e-06))\n",
    "model5.add(Dense(1))\n",
    "model5.add(Activation('sigmoid'))\n",
    "model5.compile(loss='binary_crossentropy', optimizer=adam, metrics=['accuracy',lr_metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2047136b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "58/58 [==============================] - 15s 236ms/step - loss: 0.6817 - accuracy: 0.6232 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.6608 - accuracy: 0.6727 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.6424 - accuracy: 0.6947 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.6283 - accuracy: 0.7018 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.6167 - accuracy: 0.7015 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.6085 - accuracy: 0.7044 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.6017 - accuracy: 0.7053 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.5972 - accuracy: 0.7056 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.5945 - accuracy: 0.7048 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "58/58 [==============================] - 13s 218ms/step - loss: 0.5892 - accuracy: 0.7046 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.5884 - accuracy: 0.7060 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.5821 - accuracy: 0.7091 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.5784 - accuracy: 0.7079 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.5718 - accuracy: 0.7102 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.5691 - accuracy: 0.7192 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.5639 - accuracy: 0.7170 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.5567 - accuracy: 0.7187 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.5540 - accuracy: 0.7245 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.5544 - accuracy: 0.7297 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.5462 - accuracy: 0.7302 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.5360 - accuracy: 0.7309 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.5390 - accuracy: 0.7353 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.5221 - accuracy: 0.7429 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.5342 - accuracy: 0.7428 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.5239 - accuracy: 0.7475 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.5117 - accuracy: 0.7504 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "58/58 [==============================] - 13s 218ms/step - loss: 0.5030 - accuracy: 0.7630 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.4931 - accuracy: 0.7673 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.4797 - accuracy: 0.7759 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.4763 - accuracy: 0.7773 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.4706 - accuracy: 0.7832 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.4677 - accuracy: 0.7841 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.4553 - accuracy: 0.7893 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.4488 - accuracy: 0.7951 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.4416 - accuracy: 0.8010 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.4480 - accuracy: 0.7923 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.4265 - accuracy: 0.8097 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.4257 - accuracy: 0.8095 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.4227 - accuracy: 0.8118 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.4143 - accuracy: 0.8116 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.4140 - accuracy: 0.8146 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.4108 - accuracy: 0.8217 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.4037 - accuracy: 0.8201 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "58/58 [==============================] - 12s 212ms/step - loss: 0.4037 - accuracy: 0.8184 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3983 - accuracy: 0.8215 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.4010 - accuracy: 0.8233 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.4016 - accuracy: 0.8186 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3995 - accuracy: 0.8235 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3934 - accuracy: 0.8215 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3890 - accuracy: 0.8243 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3854 - accuracy: 0.8315 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3938 - accuracy: 0.8257 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3849 - accuracy: 0.8269 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3951 - accuracy: 0.8245 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.3796 - accuracy: 0.8318 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3850 - accuracy: 0.8283 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.3776 - accuracy: 0.8377 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.3786 - accuracy: 0.8327 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3738 - accuracy: 0.8402 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.3796 - accuracy: 0.8362 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3722 - accuracy: 0.8351 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3804 - accuracy: 0.8346 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3777 - accuracy: 0.8357 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3806 - accuracy: 0.8357 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3730 - accuracy: 0.8369 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.3652 - accuracy: 0.8391 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.3778 - accuracy: 0.8364 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3643 - accuracy: 0.8395 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3717 - accuracy: 0.8409 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3661 - accuracy: 0.8465 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3648 - accuracy: 0.8426 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3671 - accuracy: 0.8416 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.3644 - accuracy: 0.8412 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3635 - accuracy: 0.8435 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3630 - accuracy: 0.8398 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.3684 - accuracy: 0.8445 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3647 - accuracy: 0.8418 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3731 - accuracy: 0.8362 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3655 - accuracy: 0.8425 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.3640 - accuracy: 0.8432 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3719 - accuracy: 0.8383 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.3611 - accuracy: 0.8416 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3577 - accuracy: 0.8480 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.3564 - accuracy: 0.8440 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3589 - accuracy: 0.8440 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.3659 - accuracy: 0.8383 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.3541 - accuracy: 0.8510 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3680 - accuracy: 0.8388 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "58/58 [==============================] - 13s 226ms/step - loss: 0.3620 - accuracy: 0.8466 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3597 - accuracy: 0.8447 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.3547 - accuracy: 0.8489 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3584 - accuracy: 0.8425 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.3552 - accuracy: 0.8486 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "58/58 [==============================] - 12s 207ms/step - loss: 0.3564 - accuracy: 0.8470 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "58/58 [==============================] - 13s 225ms/step - loss: 0.3538 - accuracy: 0.8479 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3525 - accuracy: 0.8459 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "58/58 [==============================] - 13s 225ms/step - loss: 0.3510 - accuracy: 0.8505 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "58/58 [==============================] - 13s 227ms/step - loss: 0.3521 - accuracy: 0.8484 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3604 - accuracy: 0.8407 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "58/58 [==============================] - 13s 226ms/step - loss: 0.3438 - accuracy: 0.8583 - lr: 1.0000e-04\n",
      "44/44 [==============================] - 2s 40ms/step - loss: 0.4772 - accuracy: 0.8019 - lr: 1.0000e-04\n",
      "44/44 [==============================] - 2s 41ms/step\n",
      "44/44 [==============================] - 2s 40ms/step\n",
      "Epoch 1/100\n",
      "58/58 [==============================] - 14s 220ms/step - loss: 0.6508 - accuracy: 0.6650 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.6104 - accuracy: 0.7016 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "58/58 [==============================] - 13s 218ms/step - loss: 0.6001 - accuracy: 0.7018 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.5891 - accuracy: 0.7049 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.5807 - accuracy: 0.7152 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.5679 - accuracy: 0.7194 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.5577 - accuracy: 0.7300 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.5461 - accuracy: 0.7379 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.5258 - accuracy: 0.7485 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.5092 - accuracy: 0.7579 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.5022 - accuracy: 0.7675 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.4853 - accuracy: 0.7747 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.4679 - accuracy: 0.7825 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.4784 - accuracy: 0.7686 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.4544 - accuracy: 0.7937 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.4468 - accuracy: 0.7940 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.4450 - accuracy: 0.7931 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.4424 - accuracy: 0.7947 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.4301 - accuracy: 0.8053 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.4280 - accuracy: 0.8012 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.4132 - accuracy: 0.8104 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.4211 - accuracy: 0.8074 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "58/58 [==============================] - 13s 218ms/step - loss: 0.4088 - accuracy: 0.8184 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.4129 - accuracy: 0.8074 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.4059 - accuracy: 0.8172 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "58/58 [==============================] - 13s 227ms/step - loss: 0.4014 - accuracy: 0.8170 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.3981 - accuracy: 0.8259 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3989 - accuracy: 0.8245 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.3976 - accuracy: 0.8252 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3929 - accuracy: 0.8254 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3970 - accuracy: 0.8201 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3941 - accuracy: 0.8250 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "58/58 [==============================] - 13s 225ms/step - loss: 0.3925 - accuracy: 0.8275 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3857 - accuracy: 0.8299 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.3888 - accuracy: 0.8306 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3871 - accuracy: 0.8264 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3869 - accuracy: 0.8269 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3889 - accuracy: 0.8278 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3889 - accuracy: 0.8224 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.3900 - accuracy: 0.8269 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3884 - accuracy: 0.8273 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3848 - accuracy: 0.8268 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.3857 - accuracy: 0.8299 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "58/58 [==============================] - 12s 212ms/step - loss: 0.3903 - accuracy: 0.8285 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.3847 - accuracy: 0.8273 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3990 - accuracy: 0.8191 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.3863 - accuracy: 0.8275 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.3792 - accuracy: 0.8325 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.3839 - accuracy: 0.8358 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3756 - accuracy: 0.8330 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3878 - accuracy: 0.8282 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3870 - accuracy: 0.8275 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.3819 - accuracy: 0.8327 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.3758 - accuracy: 0.8294 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3796 - accuracy: 0.8316 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.3812 - accuracy: 0.8276 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.3798 - accuracy: 0.8325 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3820 - accuracy: 0.8261 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3706 - accuracy: 0.8379 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3716 - accuracy: 0.8395 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.3782 - accuracy: 0.8299 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "58/58 [==============================] - 13s 218ms/step - loss: 0.3837 - accuracy: 0.8310 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3804 - accuracy: 0.8308 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3733 - accuracy: 0.8323 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.3717 - accuracy: 0.8383 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3768 - accuracy: 0.8323 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3717 - accuracy: 0.8371 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3752 - accuracy: 0.8365 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.3654 - accuracy: 0.8411 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3674 - accuracy: 0.8432 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3791 - accuracy: 0.8311 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3791 - accuracy: 0.8390 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3809 - accuracy: 0.8308 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.3641 - accuracy: 0.8409 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.3742 - accuracy: 0.8350 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3732 - accuracy: 0.8360 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "58/58 [==============================] - 13s 225ms/step - loss: 0.3620 - accuracy: 0.8468 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "58/58 [==============================] - 14s 243ms/step - loss: 0.3680 - accuracy: 0.8404 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "58/58 [==============================] - 14s 243ms/step - loss: 0.3636 - accuracy: 0.8435 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "58/58 [==============================] - 14s 244ms/step - loss: 0.3594 - accuracy: 0.8428 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "58/58 [==============================] - 14s 247ms/step - loss: 0.3707 - accuracy: 0.8400 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "58/58 [==============================] - 14s 237ms/step - loss: 0.3701 - accuracy: 0.8383 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "58/58 [==============================] - 14s 240ms/step - loss: 0.3639 - accuracy: 0.8442 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "58/58 [==============================] - 14s 240ms/step - loss: 0.3681 - accuracy: 0.8407 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.3612 - accuracy: 0.8468 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3715 - accuracy: 0.8358 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3627 - accuracy: 0.8414 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3637 - accuracy: 0.8404 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "58/58 [==============================] - 13s 225ms/step - loss: 0.3684 - accuracy: 0.8390 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "58/58 [==============================] - 13s 226ms/step - loss: 0.3670 - accuracy: 0.8423 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.3635 - accuracy: 0.8433 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "58/58 [==============================] - 12s 214ms/step - loss: 0.3663 - accuracy: 0.8440 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3636 - accuracy: 0.8418 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.3647 - accuracy: 0.8428 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "58/58 [==============================] - 13s 225ms/step - loss: 0.3634 - accuracy: 0.8421 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "58/58 [==============================] - 13s 226ms/step - loss: 0.3619 - accuracy: 0.8388 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3594 - accuracy: 0.8454 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3594 - accuracy: 0.8440 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.3613 - accuracy: 0.8428 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.3582 - accuracy: 0.8472 - lr: 1.0000e-04\n",
      "44/44 [==============================] - 2s 40ms/step - loss: 0.7111 - accuracy: 0.7589 - lr: 1.0000e-04\n",
      "44/44 [==============================] - 2s 41ms/step\n",
      "44/44 [==============================] - 2s 40ms/step\n",
      "Epoch 1/100\n",
      "58/58 [==============================] - 14s 213ms/step - loss: 0.6458 - accuracy: 0.6664 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "58/58 [==============================] - 13s 218ms/step - loss: 0.6027 - accuracy: 0.7039 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "58/58 [==============================] - 13s 216ms/step - loss: 0.5917 - accuracy: 0.7065 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "58/58 [==============================] - 13s 217ms/step - loss: 0.5845 - accuracy: 0.7137 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "58/58 [==============================] - 13s 217ms/step - loss: 0.5750 - accuracy: 0.7206 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "58/58 [==============================] - 12s 215ms/step - loss: 0.5673 - accuracy: 0.7267 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "58/58 [==============================] - 13s 217ms/step - loss: 0.5585 - accuracy: 0.7283 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "58/58 [==============================] - 13s 218ms/step - loss: 0.5461 - accuracy: 0.7377 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "58/58 [==============================] - 13s 218ms/step - loss: 0.5437 - accuracy: 0.7339 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "58/58 [==============================] - 13s 217ms/step - loss: 0.5346 - accuracy: 0.7433 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "58/58 [==============================] - 13s 217ms/step - loss: 0.5246 - accuracy: 0.7445 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.5121 - accuracy: 0.7537 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "58/58 [==============================] - 13s 216ms/step - loss: 0.5060 - accuracy: 0.7536 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "58/58 [==============================] - 13s 218ms/step - loss: 0.5011 - accuracy: 0.7598 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "58/58 [==============================] - 13s 218ms/step - loss: 0.4932 - accuracy: 0.7646 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.4874 - accuracy: 0.7705 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "58/58 [==============================] - 13s 218ms/step - loss: 0.4839 - accuracy: 0.7703 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.4776 - accuracy: 0.7726 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.4799 - accuracy: 0.7741 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.4713 - accuracy: 0.7762 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.4717 - accuracy: 0.7797 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.4626 - accuracy: 0.7808 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.4603 - accuracy: 0.7865 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "58/58 [==============================] - 13s 224ms/step - loss: 0.4519 - accuracy: 0.7933 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.4508 - accuracy: 0.7917 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.4544 - accuracy: 0.7874 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.4399 - accuracy: 0.7991 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "58/58 [==============================] - 13s 223ms/step - loss: 0.4225 - accuracy: 0.8083 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.4146 - accuracy: 0.8097 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.4078 - accuracy: 0.8151 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.4060 - accuracy: 0.8210 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.4136 - accuracy: 0.8113 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.4042 - accuracy: 0.8181 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.4144 - accuracy: 0.8142 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "58/58 [==============================] - 13s 219ms/step - loss: 0.3979 - accuracy: 0.8194 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "58/58 [==============================] - 13s 221ms/step - loss: 0.3981 - accuracy: 0.8189 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "58/58 [==============================] - 13s 222ms/step - loss: 0.3844 - accuracy: 0.8294 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "58/58 [==============================] - 13s 220ms/step - loss: 0.3839 - accuracy: 0.8299 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "58/58 [==============================] - 13s 217ms/step - loss: 0.3769 - accuracy: 0.8353 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "58/58 [==============================] - 13s 216ms/step - loss: 0.3858 - accuracy: 0.8266 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.3787 - accuracy: 0.8297 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "58/58 [==============================] - 10s 168ms/step - loss: 0.3869 - accuracy: 0.8310 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.3725 - accuracy: 0.8398 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "58/58 [==============================] - 10s 168ms/step - loss: 0.3873 - accuracy: 0.8259 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.3769 - accuracy: 0.8337 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.3696 - accuracy: 0.8376 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.3780 - accuracy: 0.8313 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "58/58 [==============================] - 10s 168ms/step - loss: 0.3700 - accuracy: 0.8357 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.3732 - accuracy: 0.8299 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.3738 - accuracy: 0.8374 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.3698 - accuracy: 0.8391 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "58/58 [==============================] - 10s 170ms/step - loss: 0.3631 - accuracy: 0.8438 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.3727 - accuracy: 0.8341 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.3808 - accuracy: 0.8315 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "58/58 [==============================] - 10s 170ms/step - loss: 0.3639 - accuracy: 0.8435 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "58/58 [==============================] - 10s 170ms/step - loss: 0.3651 - accuracy: 0.8421 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.3582 - accuracy: 0.8428 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.3604 - accuracy: 0.8447 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "58/58 [==============================] - 10s 170ms/step - loss: 0.3616 - accuracy: 0.8442 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "58/58 [==============================] - 10s 170ms/step - loss: 0.3601 - accuracy: 0.8383 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "58/58 [==============================] - 10s 168ms/step - loss: 0.3533 - accuracy: 0.8442 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "58/58 [==============================] - 10s 170ms/step - loss: 0.3583 - accuracy: 0.8400 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "58/58 [==============================] - 10s 167ms/step - loss: 0.3523 - accuracy: 0.8480 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.3616 - accuracy: 0.8384 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "58/58 [==============================] - 10s 170ms/step - loss: 0.3674 - accuracy: 0.8388 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.3623 - accuracy: 0.8432 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.3534 - accuracy: 0.8466 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.3534 - accuracy: 0.8480 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.3509 - accuracy: 0.8493 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.3540 - accuracy: 0.8486 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "58/58 [==============================] - 10s 170ms/step - loss: 0.3538 - accuracy: 0.8452 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "58/58 [==============================] - 10s 170ms/step - loss: 0.3522 - accuracy: 0.8442 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.3504 - accuracy: 0.8515 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "58/58 [==============================] - 10s 170ms/step - loss: 0.3458 - accuracy: 0.8499 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.3572 - accuracy: 0.8473 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.3531 - accuracy: 0.8445 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "58/58 [==============================] - 10s 170ms/step - loss: 0.3494 - accuracy: 0.8482 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "58/58 [==============================] - 10s 168ms/step - loss: 0.3471 - accuracy: 0.8543 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "58/58 [==============================] - 10s 168ms/step - loss: 0.3518 - accuracy: 0.8475 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "58/58 [==============================] - 10s 170ms/step - loss: 0.3535 - accuracy: 0.8451 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "58/58 [==============================] - 10s 170ms/step - loss: 0.3455 - accuracy: 0.8557 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "58/58 [==============================] - 10s 168ms/step - loss: 0.3445 - accuracy: 0.8559 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.3405 - accuracy: 0.8531 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.3475 - accuracy: 0.8480 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "58/58 [==============================] - 10s 170ms/step - loss: 0.3429 - accuracy: 0.8512 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.3501 - accuracy: 0.8491 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "58/58 [==============================] - 10s 168ms/step - loss: 0.3470 - accuracy: 0.8519 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "58/58 [==============================] - 10s 170ms/step - loss: 0.3525 - accuracy: 0.8473 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "58/58 [==============================] - 10s 168ms/step - loss: 0.3422 - accuracy: 0.8501 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.3471 - accuracy: 0.8477 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.3417 - accuracy: 0.8520 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "58/58 [==============================] - 10s 168ms/step - loss: 0.3463 - accuracy: 0.8499 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.3399 - accuracy: 0.8499 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.3426 - accuracy: 0.8543 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.3417 - accuracy: 0.8554 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.3429 - accuracy: 0.8564 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "58/58 [==============================] - 10s 167ms/step - loss: 0.3431 - accuracy: 0.8527 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.3402 - accuracy: 0.8557 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "58/58 [==============================] - 10s 170ms/step - loss: 0.3424 - accuracy: 0.8541 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.3449 - accuracy: 0.8512 - lr: 1.0000e-04\n",
      "44/44 [==============================] - 2s 34ms/step - loss: 1.2529 - accuracy: 0.7139 - lr: 1.0000e-04\n",
      "44/44 [==============================] - 2s 34ms/step\n",
      "44/44 [==============================] - 1s 33ms/step\n",
      "Epoch 1/100\n",
      "58/58 [==============================] - 11s 163ms/step - loss: 0.6473 - accuracy: 0.6675 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.6093 - accuracy: 0.7029 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.6015 - accuracy: 0.7034 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "58/58 [==============================] - 10s 170ms/step - loss: 0.5987 - accuracy: 0.7055 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "58/58 [==============================] - 10s 172ms/step - loss: 0.5936 - accuracy: 0.7081 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.5854 - accuracy: 0.7095 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.5806 - accuracy: 0.7184 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.5729 - accuracy: 0.7206 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.5658 - accuracy: 0.7252 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.5560 - accuracy: 0.7304 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "58/58 [==============================] - 10s 170ms/step - loss: 0.5555 - accuracy: 0.7365 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.5456 - accuracy: 0.7400 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.5361 - accuracy: 0.7421 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "58/58 [==============================] - 10s 169ms/step - loss: 0.5277 - accuracy: 0.7443 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.5232 - accuracy: 0.7482 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.5165 - accuracy: 0.7522 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.5090 - accuracy: 0.7548 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.4970 - accuracy: 0.7628 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.4933 - accuracy: 0.7642 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.4924 - accuracy: 0.7619 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "58/58 [==============================] - 10s 172ms/step - loss: 0.4843 - accuracy: 0.7684 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.4806 - accuracy: 0.7689 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.4762 - accuracy: 0.7764 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "58/58 [==============================] - 10s 175ms/step - loss: 0.4726 - accuracy: 0.7726 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.4590 - accuracy: 0.7846 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.4562 - accuracy: 0.7858 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "58/58 [==============================] - 10s 172ms/step - loss: 0.4524 - accuracy: 0.7921 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.4384 - accuracy: 0.7933 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.4420 - accuracy: 0.7984 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "58/58 [==============================] - 10s 172ms/step - loss: 0.4261 - accuracy: 0.8025 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "58/58 [==============================] - 10s 176ms/step - loss: 0.4207 - accuracy: 0.8111 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.4239 - accuracy: 0.8041 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "58/58 [==============================] - 10s 172ms/step - loss: 0.4167 - accuracy: 0.8114 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "58/58 [==============================] - 10s 172ms/step - loss: 0.4061 - accuracy: 0.8146 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "58/58 [==============================] - 10s 172ms/step - loss: 0.4084 - accuracy: 0.8170 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.4147 - accuracy: 0.8139 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "58/58 [==============================] - 10s 176ms/step - loss: 0.3974 - accuracy: 0.8200 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.3923 - accuracy: 0.8273 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3966 - accuracy: 0.8175 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3920 - accuracy: 0.8215 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.3829 - accuracy: 0.8287 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "58/58 [==============================] - 10s 175ms/step - loss: 0.3828 - accuracy: 0.8262 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "58/58 [==============================] - 10s 175ms/step - loss: 0.3966 - accuracy: 0.8233 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3781 - accuracy: 0.8306 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3730 - accuracy: 0.8381 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3781 - accuracy: 0.8259 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.3734 - accuracy: 0.8360 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "58/58 [==============================] - 10s 175ms/step - loss: 0.3749 - accuracy: 0.8318 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.3680 - accuracy: 0.8407 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3672 - accuracy: 0.8371 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "58/58 [==============================] - 10s 175ms/step - loss: 0.3671 - accuracy: 0.8365 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.3665 - accuracy: 0.8414 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.3626 - accuracy: 0.8444 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.3606 - accuracy: 0.8426 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3649 - accuracy: 0.8400 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "58/58 [==============================] - 10s 175ms/step - loss: 0.3598 - accuracy: 0.8405 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "58/58 [==============================] - 10s 172ms/step - loss: 0.3641 - accuracy: 0.8412 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.3554 - accuracy: 0.8433 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3727 - accuracy: 0.8360 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "58/58 [==============================] - 10s 172ms/step - loss: 0.3657 - accuracy: 0.8388 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "58/58 [==============================] - 10s 175ms/step - loss: 0.3627 - accuracy: 0.8398 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3520 - accuracy: 0.8451 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "58/58 [==============================] - 10s 175ms/step - loss: 0.3589 - accuracy: 0.8479 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "58/58 [==============================] - 10s 175ms/step - loss: 0.3577 - accuracy: 0.8433 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.3538 - accuracy: 0.8444 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.3505 - accuracy: 0.8468 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.3472 - accuracy: 0.8479 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3652 - accuracy: 0.8402 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "58/58 [==============================] - 10s 175ms/step - loss: 0.3500 - accuracy: 0.8475 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.3500 - accuracy: 0.8512 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3699 - accuracy: 0.8364 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "58/58 [==============================] - 10s 175ms/step - loss: 0.3576 - accuracy: 0.8433 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3480 - accuracy: 0.8499 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "58/58 [==============================] - 10s 176ms/step - loss: 0.3527 - accuracy: 0.8449 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3451 - accuracy: 0.8536 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3471 - accuracy: 0.8517 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "58/58 [==============================] - 10s 175ms/step - loss: 0.3520 - accuracy: 0.8517 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "58/58 [==============================] - 10s 175ms/step - loss: 0.3465 - accuracy: 0.8468 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.3462 - accuracy: 0.8522 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3494 - accuracy: 0.8473 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3540 - accuracy: 0.8452 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3450 - accuracy: 0.8526 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3416 - accuracy: 0.8529 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.3503 - accuracy: 0.8482 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.3444 - accuracy: 0.8489 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.3475 - accuracy: 0.8499 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "58/58 [==============================] - 10s 172ms/step - loss: 0.3411 - accuracy: 0.8550 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3507 - accuracy: 0.8475 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.3422 - accuracy: 0.8513 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3456 - accuracy: 0.8560 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "58/58 [==============================] - 10s 176ms/step - loss: 0.3491 - accuracy: 0.8534 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3449 - accuracy: 0.8491 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "58/58 [==============================] - 10s 175ms/step - loss: 0.3423 - accuracy: 0.8480 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3468 - accuracy: 0.8466 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3505 - accuracy: 0.8508 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.3467 - accuracy: 0.8501 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3384 - accuracy: 0.8580 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3453 - accuracy: 0.8501 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "58/58 [==============================] - 10s 176ms/step - loss: 0.3376 - accuracy: 0.8548 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.3376 - accuracy: 0.8557 - lr: 1.0000e-04\n",
      "44/44 [==============================] - 2s 34ms/step - loss: 1.9004 - accuracy: 0.7060 - lr: 1.0000e-04\n",
      "44/44 [==============================] - 2s 33ms/step\n",
      "44/44 [==============================] - 1s 34ms/step\n",
      "Epoch 1/100\n",
      "58/58 [==============================] - 11s 165ms/step - loss: 0.6608 - accuracy: 0.6642 - lr: 1.0000e-04\n",
      "Epoch 2/100\n",
      "58/58 [==============================] - 10s 172ms/step - loss: 0.6085 - accuracy: 0.7011 - lr: 1.0000e-04\n",
      "Epoch 3/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.6010 - accuracy: 0.7013 - lr: 1.0000e-04\n",
      "Epoch 4/100\n",
      "58/58 [==============================] - 10s 172ms/step - loss: 0.5957 - accuracy: 0.7020 - lr: 1.0000e-04\n",
      "Epoch 5/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.5904 - accuracy: 0.7039 - lr: 1.0000e-04\n",
      "Epoch 6/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.5735 - accuracy: 0.7048 - lr: 1.0000e-04\n",
      "Epoch 7/100\n",
      "58/58 [==============================] - 10s 171ms/step - loss: 0.5477 - accuracy: 0.7274 - lr: 1.0000e-04\n",
      "Epoch 8/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.5282 - accuracy: 0.7459 - lr: 1.0000e-04\n",
      "Epoch 9/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.5142 - accuracy: 0.7543 - lr: 1.0000e-04\n",
      "Epoch 10/100\n",
      "58/58 [==============================] - 10s 174ms/step - loss: 0.4946 - accuracy: 0.7670 - lr: 1.0000e-04\n",
      "Epoch 11/100\n",
      "58/58 [==============================] - 10s 173ms/step - loss: 0.4762 - accuracy: 0.7804 - lr: 1.0000e-04\n",
      "Epoch 12/100\n",
      "58/58 [==============================] - 10s 177ms/step - loss: 0.4782 - accuracy: 0.7804 - lr: 1.0000e-04\n",
      "Epoch 13/100\n",
      "58/58 [==============================] - 10s 178ms/step - loss: 0.4692 - accuracy: 0.7842 - lr: 1.0000e-04\n",
      "Epoch 14/100\n",
      "58/58 [==============================] - 10s 178ms/step - loss: 0.4667 - accuracy: 0.7937 - lr: 1.0000e-04\n",
      "Epoch 15/100\n",
      "58/58 [==============================] - 10s 175ms/step - loss: 0.4665 - accuracy: 0.7891 - lr: 1.0000e-04\n",
      "Epoch 16/100\n",
      "58/58 [==============================] - 10s 177ms/step - loss: 0.4606 - accuracy: 0.7951 - lr: 1.0000e-04\n",
      "Epoch 17/100\n",
      "58/58 [==============================] - 10s 176ms/step - loss: 0.4560 - accuracy: 0.7919 - lr: 1.0000e-04\n",
      "Epoch 18/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.4504 - accuracy: 0.7957 - lr: 1.0000e-04\n",
      "Epoch 19/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.4553 - accuracy: 0.7924 - lr: 1.0000e-04\n",
      "Epoch 20/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.4467 - accuracy: 0.7989 - lr: 1.0000e-04\n",
      "Epoch 21/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.4472 - accuracy: 0.7977 - lr: 1.0000e-04\n",
      "Epoch 22/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.4477 - accuracy: 0.7933 - lr: 1.0000e-04\n",
      "Epoch 23/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.4429 - accuracy: 0.8022 - lr: 1.0000e-04\n",
      "Epoch 24/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.4454 - accuracy: 0.7996 - lr: 1.0000e-04\n",
      "Epoch 25/100\n",
      "58/58 [==============================] - 10s 181ms/step - loss: 0.4466 - accuracy: 0.8003 - lr: 1.0000e-04\n",
      "Epoch 26/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.4411 - accuracy: 0.7989 - lr: 1.0000e-04\n",
      "Epoch 27/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.4364 - accuracy: 0.8008 - lr: 1.0000e-04\n",
      "Epoch 28/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.4346 - accuracy: 0.8066 - lr: 1.0000e-04\n",
      "Epoch 29/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.4324 - accuracy: 0.8078 - lr: 1.0000e-04\n",
      "Epoch 30/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.4337 - accuracy: 0.8048 - lr: 1.0000e-04\n",
      "Epoch 31/100\n",
      "58/58 [==============================] - 10s 181ms/step - loss: 0.4398 - accuracy: 0.7982 - lr: 1.0000e-04\n",
      "Epoch 32/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.4294 - accuracy: 0.8085 - lr: 1.0000e-04\n",
      "Epoch 33/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.4334 - accuracy: 0.8015 - lr: 1.0000e-04\n",
      "Epoch 34/100\n",
      "58/58 [==============================] - 11s 181ms/step - loss: 0.4310 - accuracy: 0.8008 - lr: 1.0000e-04\n",
      "Epoch 35/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.4257 - accuracy: 0.8120 - lr: 1.0000e-04\n",
      "Epoch 36/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.4262 - accuracy: 0.8076 - lr: 1.0000e-04\n",
      "Epoch 37/100\n",
      "58/58 [==============================] - 11s 182ms/step - loss: 0.4235 - accuracy: 0.8093 - lr: 1.0000e-04\n",
      "Epoch 38/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.4199 - accuracy: 0.8085 - lr: 1.0000e-04\n",
      "Epoch 39/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.4168 - accuracy: 0.8121 - lr: 1.0000e-04\n",
      "Epoch 40/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.4214 - accuracy: 0.8079 - lr: 1.0000e-04\n",
      "Epoch 41/100\n",
      "58/58 [==============================] - 11s 181ms/step - loss: 0.4140 - accuracy: 0.8140 - lr: 1.0000e-04\n",
      "Epoch 42/100\n",
      "58/58 [==============================] - 11s 181ms/step - loss: 0.4114 - accuracy: 0.8175 - lr: 1.0000e-04\n",
      "Epoch 43/100\n",
      "58/58 [==============================] - 10s 181ms/step - loss: 0.4214 - accuracy: 0.8102 - lr: 1.0000e-04\n",
      "Epoch 44/100\n",
      "58/58 [==============================] - 11s 182ms/step - loss: 0.4169 - accuracy: 0.8130 - lr: 1.0000e-04\n",
      "Epoch 45/100\n",
      "58/58 [==============================] - 11s 182ms/step - loss: 0.4181 - accuracy: 0.8132 - lr: 1.0000e-04\n",
      "Epoch 46/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.4109 - accuracy: 0.8182 - lr: 1.0000e-04\n",
      "Epoch 47/100\n",
      "58/58 [==============================] - 11s 182ms/step - loss: 0.4121 - accuracy: 0.8149 - lr: 1.0000e-04\n",
      "Epoch 48/100\n",
      "58/58 [==============================] - 10s 181ms/step - loss: 0.4071 - accuracy: 0.8161 - lr: 1.0000e-04\n",
      "Epoch 49/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.4111 - accuracy: 0.8125 - lr: 1.0000e-04\n",
      "Epoch 50/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.4158 - accuracy: 0.8088 - lr: 1.0000e-04\n",
      "Epoch 51/100\n",
      "58/58 [==============================] - 11s 182ms/step - loss: 0.4036 - accuracy: 0.8212 - lr: 1.0000e-04\n",
      "Epoch 52/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.4065 - accuracy: 0.8125 - lr: 1.0000e-04\n",
      "Epoch 53/100\n",
      "58/58 [==============================] - 11s 181ms/step - loss: 0.4033 - accuracy: 0.8160 - lr: 1.0000e-04\n",
      "Epoch 54/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.4048 - accuracy: 0.8217 - lr: 1.0000e-04\n",
      "Epoch 55/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.4074 - accuracy: 0.8142 - lr: 1.0000e-04\n",
      "Epoch 56/100\n",
      "58/58 [==============================] - 11s 181ms/step - loss: 0.4044 - accuracy: 0.8229 - lr: 1.0000e-04\n",
      "Epoch 57/100\n",
      "58/58 [==============================] - 10s 181ms/step - loss: 0.4047 - accuracy: 0.8177 - lr: 1.0000e-04\n",
      "Epoch 58/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.4052 - accuracy: 0.8191 - lr: 1.0000e-04\n",
      "Epoch 59/100\n",
      "58/58 [==============================] - 11s 182ms/step - loss: 0.3985 - accuracy: 0.8217 - lr: 1.0000e-04\n",
      "Epoch 60/100\n",
      "58/58 [==============================] - 11s 182ms/step - loss: 0.3957 - accuracy: 0.8222 - lr: 1.0000e-04\n",
      "Epoch 61/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.4017 - accuracy: 0.8198 - lr: 1.0000e-04\n",
      "Epoch 62/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.4003 - accuracy: 0.8233 - lr: 1.0000e-04\n",
      "Epoch 63/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.3976 - accuracy: 0.8201 - lr: 1.0000e-04\n",
      "Epoch 64/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.4011 - accuracy: 0.8200 - lr: 1.0000e-04\n",
      "Epoch 65/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.3953 - accuracy: 0.8233 - lr: 1.0000e-04\n",
      "Epoch 66/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.3969 - accuracy: 0.8212 - lr: 1.0000e-04\n",
      "Epoch 67/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.3932 - accuracy: 0.8205 - lr: 1.0000e-04\n",
      "Epoch 68/100\n",
      "58/58 [==============================] - 11s 183ms/step - loss: 0.3884 - accuracy: 0.8282 - lr: 1.0000e-04\n",
      "Epoch 69/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.3900 - accuracy: 0.8285 - lr: 1.0000e-04\n",
      "Epoch 70/100\n",
      "58/58 [==============================] - 10s 181ms/step - loss: 0.3952 - accuracy: 0.8249 - lr: 1.0000e-04\n",
      "Epoch 71/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.3950 - accuracy: 0.8245 - lr: 1.0000e-04\n",
      "Epoch 72/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.3947 - accuracy: 0.8264 - lr: 1.0000e-04\n",
      "Epoch 73/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.3929 - accuracy: 0.8247 - lr: 1.0000e-04\n",
      "Epoch 74/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.3909 - accuracy: 0.8243 - lr: 1.0000e-04\n",
      "Epoch 75/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.3921 - accuracy: 0.8247 - lr: 1.0000e-04\n",
      "Epoch 76/100\n",
      "58/58 [==============================] - 11s 181ms/step - loss: 0.3847 - accuracy: 0.8334 - lr: 1.0000e-04\n",
      "Epoch 77/100\n",
      "58/58 [==============================] - 11s 182ms/step - loss: 0.3879 - accuracy: 0.8247 - lr: 1.0000e-04\n",
      "Epoch 78/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.3860 - accuracy: 0.8280 - lr: 1.0000e-04\n",
      "Epoch 79/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.3813 - accuracy: 0.8290 - lr: 1.0000e-04\n",
      "Epoch 80/100\n",
      "58/58 [==============================] - 10s 181ms/step - loss: 0.3865 - accuracy: 0.8315 - lr: 1.0000e-04\n",
      "Epoch 81/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.3933 - accuracy: 0.8249 - lr: 1.0000e-04\n",
      "Epoch 82/100\n",
      "58/58 [==============================] - 11s 181ms/step - loss: 0.3830 - accuracy: 0.8322 - lr: 1.0000e-04\n",
      "Epoch 83/100\n",
      "58/58 [==============================] - 11s 181ms/step - loss: 0.3779 - accuracy: 0.8341 - lr: 1.0000e-04\n",
      "Epoch 84/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.3838 - accuracy: 0.8294 - lr: 1.0000e-04\n",
      "Epoch 85/100\n",
      "58/58 [==============================] - 11s 182ms/step - loss: 0.3848 - accuracy: 0.8322 - lr: 1.0000e-04\n",
      "Epoch 86/100\n",
      "58/58 [==============================] - 10s 181ms/step - loss: 0.3870 - accuracy: 0.8247 - lr: 1.0000e-04\n",
      "Epoch 87/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.3862 - accuracy: 0.8327 - lr: 1.0000e-04\n",
      "Epoch 88/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.3792 - accuracy: 0.8351 - lr: 1.0000e-04\n",
      "Epoch 89/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.3773 - accuracy: 0.8355 - lr: 1.0000e-04\n",
      "Epoch 90/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.3796 - accuracy: 0.8339 - lr: 1.0000e-04\n",
      "Epoch 91/100\n",
      "58/58 [==============================] - 11s 182ms/step - loss: 0.3870 - accuracy: 0.8255 - lr: 1.0000e-04\n",
      "Epoch 92/100\n",
      "58/58 [==============================] - 11s 182ms/step - loss: 0.3764 - accuracy: 0.8330 - lr: 1.0000e-04\n",
      "Epoch 93/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.3768 - accuracy: 0.8341 - lr: 1.0000e-04\n",
      "Epoch 94/100\n",
      "58/58 [==============================] - 11s 182ms/step - loss: 0.3759 - accuracy: 0.8339 - lr: 1.0000e-04\n",
      "Epoch 95/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.3807 - accuracy: 0.8292 - lr: 1.0000e-04\n",
      "Epoch 96/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.3732 - accuracy: 0.8325 - lr: 1.0000e-04\n",
      "Epoch 97/100\n",
      "58/58 [==============================] - 10s 181ms/step - loss: 0.3743 - accuracy: 0.8393 - lr: 1.0000e-04\n",
      "Epoch 98/100\n",
      "58/58 [==============================] - 10s 179ms/step - loss: 0.3760 - accuracy: 0.8362 - lr: 1.0000e-04\n",
      "Epoch 99/100\n",
      "58/58 [==============================] - 10s 180ms/step - loss: 0.3811 - accuracy: 0.8329 - lr: 1.0000e-04\n",
      "Epoch 100/100\n",
      "58/58 [==============================] - 11s 182ms/step - loss: 0.3764 - accuracy: 0.8351 - lr: 1.0000e-04\n",
      "44/44 [==============================] - 2s 32ms/step - loss: 0.4282 - accuracy: 0.8033 - lr: 1.0000e-04\n",
      "44/44 [==============================] - 2s 32ms/step\n",
      "44/44 [==============================] - 1s 32ms/step\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "tprs=[]\n",
    "mean_fpr=np.linspace(0,1,100)\n",
    "auck=[0,0,0,0,0]\n",
    "Recalls=[]\n",
    "F1score=[]\n",
    "specificity=[]\n",
    "accuracys = []\n",
    "for i in range(5):\n",
    "    if i==0:\n",
    "        x_train, y_train = x_train0,y_train0\n",
    "        x_test, y_test = x_test0,y_test0\n",
    "        model.fit(x_train, y_train,batch_size=100,epochs=100)\n",
    "        score = model.evaluate(x_test, y_test)[1]\n",
    "        scores.append(score)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test,model.predict(x_test))\n",
    "        y_pred=model.predict(x_test)\n",
    "        y_pred = (y_pred >= 0.5)*1  \n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        specificity.append(tn / (tn + fp))\n",
    "        accuracys.append(accuracy_score(y_test, y_pred))\n",
    "        Recalls.append(metrics.recall_score(y_test, y_pred))\n",
    "        F1score.append(metrics.f1_score(y_test, y_pred))\n",
    "    elif i ==1:\n",
    "        x_train, y_train = x_train1,y_train1\n",
    "        x_test, y_test = x_test1,y_test1\n",
    "        model2.fit(x_train, y_train,batch_size=100,epochs=100)\n",
    "        score = model2.evaluate(x_test, y_test)[1]\n",
    "        scores.append(score)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test,model2.predict(x_test))\n",
    "        y_pred=model2.predict(x_test)\n",
    "        y_pred = (y_pred >= 0.5)*1  \n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        specificity.append(tn / (tn + fp))\n",
    "        accuracys.append(accuracy_score(y_test, y_pred))\n",
    "        Recalls.append(metrics.recall_score(y_test, y_pred))\n",
    "        F1score.append(metrics.f1_score(y_test, y_pred))\n",
    "    elif i ==2:\n",
    "        x_train, y_train = x_train2,y_train2\n",
    "        x_test, y_test = x_test2,y_test2\n",
    "        model3.fit(x_train, y_train,batch_size=100,epochs=100)\n",
    "        score = model3.evaluate(x_test, y_test)[1]\n",
    "        scores.append(score)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test,model3.predict(x_test))\n",
    "        y_pred=model3.predict(x_test)\n",
    "        y_pred = (y_pred >= 0.5)*1  \n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        specificity.append(tn / (tn + fp))\n",
    "        accuracys.append(accuracy_score(y_test, y_pred))\n",
    "        Recalls.append(metrics.recall_score(y_test, y_pred))\n",
    "        F1score.append(metrics.f1_score(y_test, y_pred))\n",
    "    elif i ==3:\n",
    "        x_train, y_train = x_train3,y_train3\n",
    "        x_test, y_test = x_test3,y_test3\n",
    "        model4.fit(x_train, y_train,batch_size=100,epochs=100)\n",
    "        score = model4.evaluate(x_test, y_test)[1]\n",
    "        scores.append(score)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test,model4.predict(x_test))\n",
    "        y_pred=model4.predict(x_test)\n",
    "        y_pred = (y_pred >= 0.5)*1  \n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        specificity.append(tn / (tn + fp))\n",
    "        accuracys.append(accuracy_score(y_test, y_pred))\n",
    "        Recalls.append(metrics.recall_score(y_test, y_pred))\n",
    "        F1score.append(metrics.f1_score(y_test, y_pred))\n",
    "    else:\n",
    "        x_train, y_train = x_train4,y_train4\n",
    "        x_test, y_test = x_test4,y_test4\n",
    "        model5.fit(x_train, y_train,batch_size=100,epochs=100)\n",
    "        score = model5.evaluate(x_test, y_test)[1]\n",
    "        scores.append(score)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test,model5.predict(x_test))\n",
    "        y_pred=model5.predict(x_test)\n",
    "        y_pred = (y_pred >= 0.5)*1  \n",
    "        tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "        specificity.append(tn / (tn + fp))\n",
    "        accuracys.append(accuracy_score(y_test, y_pred))\n",
    "        Recalls.append(metrics.recall_score(y_test, y_pred))\n",
    "        F1score.append(metrics.f1_score(y_test, y_pred))\n",
    "    auck[i] = auc(fpr, tpr)\n",
    "    tprs.append(interp(mean_fpr,fpr,tpr))\n",
    "    tprs[-1][0]=0.0\n",
    "scores=np.array(scores)\n",
    "interval = stats.t.interval(0.95, scores.shape[0] - 1, scores.mean(), scores.std() / np.sqrt(scores.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc5b3882",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confidence interval：(0.7052447397388728, 0.8083461206279485)\n",
      "scores: 0.7567954301834107\n",
      "AUC: 0.8895203109424124\n"
     ]
    }
   ],
   "source": [
    "print(\"confidence interval：{}\".format(interval))\n",
    "print('scores:',np.mean(scores))\n",
    "print('AUC:',np.mean(auck))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53c4f776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recalls: 0.9565217391304348\n",
      "F1score: 0.8451759603213077\n"
     ]
    }
   ],
   "source": [
    "print('Recalls:',np.mean(Recalls))\n",
    "print('F1score:',np.mean(F1score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e404e0dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.801859799713877, 0.7589413447782547, 0.7138769670958512, 0.7060085836909872, 0.8032904148783977]\n",
      "Accuracys: 0.7567954220314735\n"
     ]
    }
   ],
   "source": [
    "print(accuracys)\n",
    "print('Accuracys:',np.mean(accuracys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b671583a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x214738cfb20>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.xlim([-0.05,1.05])\n",
    "plt.ylim([-0.05,1.05])\n",
    "plt.grid(alpha=0.4)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.set_aspect(1)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.plot([0,1],[0,1],linestyle='--',lw=2,color='r',alpha=0.1)\n",
    "plt.xlabel('False Positive Rate', fontsize=13)\n",
    "plt.ylabel('True Positive Rate', fontsize=13)\n",
    "mean_tpr=np.mean(tprs,axis=0)\n",
    "mean_tpr[-1]=1.0\n",
    "std_auc=np.std(tprs,axis=0)\n",
    "plt.plot(mean_fpr,mean_tpr,color='b',label = u'AUC=%.3f' % (sum(auck)/5),lw=2,alpha=0.7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 | packaged by Anaconda, Inc. | (main, Mar  8 2023, 10:42:25) [MSC v.1916 64 bit (AMD64)]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "3c06e3e46abf38078fe4dac36a0085ec2b134ebbd73dd076183d243eeca6918f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
